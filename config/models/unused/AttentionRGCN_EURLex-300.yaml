name: AttentionRGCN

model:
  num_layers: 1
  hidden_size: 256
  gcn_hidden_size: [128, 64]
  linear_size: [32]
  dropout: 0.5
  max_length: 200
  emb_trainable: false
  emb_size: 300
  emb_init: data/EURLex/emb_init.npy  # EURLex-300

train:
  input_opts:
    return_attn: true
  gen_input_opts:
    pass_attn: true
    rnn_training: true
  last_input_opts:
    pass_attn: true
